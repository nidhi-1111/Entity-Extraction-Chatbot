{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "(3) This script saves the SBERT model and embeddings into a pickle file:\n",
        "\n",
        "1. Initializes NLTK tools for tokenization, stemming, and lemmatization.\n",
        "2. Loads a pre-trained SBERT model for generating sentence embeddings.\n",
        "3. Preprocesses FAQ questions (tokenization, stemming, lemmatization, and lowercasing).\n",
        "4. Computes embeddings for FAQ questions.\n",
        "5. Saves the model, FAQ questions, FAQ embeddings, and FAQs to a pickle file in a specified directory.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6w-Di2Mwtah0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsbJRVMntd_A",
        "outputId": "b4b0485b-d16d-475c-e303-962a359d8bbf"
      },
      "outputs": [],
      "source": [
        "# Load NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRwEab_ltecS"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained SBERT model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Preprocess text (tokenization, stemming, lemmatization, and lowercasing)\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove non-alphanumeric characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenize text\n",
        "    tokens = [stemmer.stem(word) for word in tokens]  # Apply stemming\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Apply lemmatization\n",
        "    return ' '.join(tokens)  # Join tokens back into a single string\n",
        "\n",
        "# Generate SBERT embeddings\n",
        "def get_sbert_embedding(text):\n",
        "    embedding = model.encode(text)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the FAQs from the JSON file\n",
        "with open('data/keelworks_info.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "faqs = data['questions_and_answers']\n",
        "\n",
        "# Precompute embeddings for FAQ questions\n",
        "faq_questions = [preprocess_text(faq['question']) for faq in faqs]\n",
        "faq_embeddings = np.array([get_sbert_embedding(question) for question in faq_questions])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2RMGfUAtmkq",
        "outputId": "d106efd1-4852-43a4-f6a5-7738acdc6fb3"
      },
      "outputs": [],
      "source": [
        "# Define the directory and file name\n",
        "model_directory = 'model'\n",
        "file_name = 'keelworks_model.pkl'\n",
        "file_path = os.path.join(model_directory, file_name)\n",
        "\n",
        "# Save model and embeddings to a pickle file\n",
        "model_data = {\n",
        "    'model': model,\n",
        "    'faq_questions': faq_questions,\n",
        "    'faq_embeddings': faq_embeddings,\n",
        "    'faqs': faqs\n",
        "}\n",
        "\n",
        "with open(file_path, 'wb') as f:\n",
        "    pickle.dump(model_data, f)\n",
        "\n",
        "print(f\"Model and embeddings saved to {file_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
