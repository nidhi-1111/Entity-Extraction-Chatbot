{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "(4) This script executes the SBERT model chatbot from the pickle file:\n",
        "\n",
        "1. Loads a pre-trained model and embeddings from a pickle file.\n",
        "2. Preprocesses user queries by tokenizing, stemming, lemmatizing, and lowercasing the text.\n",
        "3. Computes embeddings for user queries.\n",
        "4. Finds the most relevant FAQ answer using cosine similarity.\n",
        "5. Provides a command-line interface for user interaction.\n",
        "The chatbot responds to user queries based on precomputed FAQ data and embeddings. If no suitable answer is found, it provides a default message.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcE22bUTt3IS"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwHcvDthuLUq",
        "outputId": "829977c3-53b7-435e-8fe1-77441c2ae5f5"
      },
      "outputs": [],
      "source": [
        "# Load NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and embeddings from pickle file\n",
        "with open('model/keelworks_model.pkl', 'rb') as f:\n",
        "    model_data = pickle.load(f)\n",
        "\n",
        "model = model_data['model']\n",
        "faqs = model_data['faqs']\n",
        "faq_questions = model_data['faq_questions']\n",
        "faq_embeddings = model_data['faq_embeddings']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZNB5ZqLuUgJ"
      },
      "outputs": [],
      "source": [
        "# Preprocess text (tokenization, stemming, lemmatization, and lowercasing)\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove non-alphanumeric characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    tokens = word_tokenize(text)  # Tokenize text\n",
        "    tokens = [stemmer.stem(word) for word in tokens]  # Apply stemming\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Apply lemmatization\n",
        "    return ' '.join(tokens)  # Join tokens back into a single string\n",
        "\n",
        "# Generate SBERT embeddings\n",
        "def get_sbert_embedding(text):\n",
        "    embedding = model.encode(text)\n",
        "    return embedding\n",
        "\n",
        "# Find the best matching answer\n",
        "def get_best_answer(user_query, faqs, faq_embeddings, threshold=0.5):\n",
        "    preprocessed_query = preprocess_text(user_query)\n",
        "    query_embedding = get_sbert_embedding(preprocessed_query).reshape(1, -1)\n",
        "\n",
        "    similarities = cosine_similarity(query_embedding, faq_embeddings)\n",
        "    best_match_index = similarities.argmax()\n",
        "    best_match_score = similarities[0, best_match_index]\n",
        "\n",
        "    if best_match_score < threshold:\n",
        "        return \"Sorry, I don't have the answer. Please email to test@keelworks to get more info.\"\n",
        "    \n",
        "    return faqs[best_match_index]['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUzDKDvgvkmL",
        "outputId": "85b49d16-9ea4-4667-e7f5-63da6de80aaf"
      },
      "outputs": [],
      "source": [
        "# Command-Line Interface\n",
        "def chatbot():\n",
        "    print(\"Welcome to the KeelWorks Chatbot!\")\n",
        "    user_name = input(\"Please enter your name: \")\n",
        "    print(f\"Hello {user_name}, welcome to the KeelWorks bot. Ask me anything about KeelWorks.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nYou: \")\n",
        "        if user_query.lower() in ['exit', 'quit', 'bye']:\n",
        "            print(f\"Goodbye, {user_name}!\")\n",
        "            break\n",
        "        answer = get_best_answer(user_query, faqs, faq_embeddings)\n",
        "        print(f\"Bot: {answer}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    chatbot()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
